{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bc1987",
   "metadata": {},
   "source": [
    "### TODO: Feature Engineering\n",
    "\n",
    "\"#### Primary tasks (must-have)\\n\",\n",
    "- [ ] Document which raw columns are dropped/kept and why (e.g., `high_corr_features` list).\n",
    "- [ ] Encode categoricals consistently—Tip: stash the fitted encoder (e.g., `OneHotEncoder(handle_unknown=\"ignore\")`) so inference matches training.\n",
    "- [ ] Normalize/standardize numeric ranges (`StandardScaler`, `MinMaxScaler`) and persist the scaler params.\n",
    "- [ ] Capture any derived features (aggregations, ratios) with short examples so later notebooks can regenerate them.\n",
    "- [ ] Drop Socket/Identity columns (IPV4_SRC_ADDR, IPV4_DST_ADDR, L4_SRC_PORT, Timestamp) to prevent data leakage and model memorization.\n",
    "- [ ] Apply Log Transformation ($log1p$) to heavily skewed numeric features (e.g., IN_BYTES, OUT_BYTES, FLOW_DURATION_MS) before scaling.\n",
    "- [ ] Implement a Class Balancing strategy (e.g., SMOTE or Class Weights) to address the extreme scarcity of attack labels compared to benign traffic.\n",
    "- [ ] Lock Test/Val sets: Isolate the test and validation sets immediately after the stratified split; they should never \"see\" a balancer like SMOTE.\n",
    "- [ ] Fit only on Train: Call .fit() strictly on the training split for all scalers and encoders to establish the processing \"rules.\"\n",
    "- [ ] Transform-only for Val/Test: Use .transform() to apply the training \"rules\" to the validation and test sets without recalculating parameters.\n",
    "- [ ] Oversample post-split: Apply SMOTE or balancing techniques strictly to the training data to avoid creating synthetic duplicates of your test samples.\n",
    "\n",
    "\"#### Secondary tasks (second essentials)\\n\",\n",
    "- [ ] Attach MITRE ATT&CK tactic/technique tags as engineered features (Tip: keep a keyword→technique mapping dict and serialize it for inference).\n",
    "- [ ] [Supervised ML] Preserve TF-IDF vocabulary/features used by the RandomForest text pipeline so SHAP explains identical dimensions (Tip: dump `tfidf.get_feature_names_out()` with the encoder artifacts).\n",
    "- [ ] [Unsupervised ML] Assemble the Isolation-Forest feature frame (`Hour`, `Src_IP_LastOctet`, etc.) and record any imputations so runtime anomaly detection feeds match training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff31e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset from the Google Drive\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
