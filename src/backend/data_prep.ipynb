{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83eb2920",
   "metadata": {},
   "source": [
    "### TODO: Data Preprocessing\n",
    "\n",
    "#### Primary tasks (must-have)\n",
    "- [ ] Load & Clean the NF-UNSW-NB15-v3 dataset (or skip if confirmed pre-cleaned) and log basic stats via `df.shape` / `df.info()`.\n",
    "- [ ] Load the synthetic data for checking \n",
    "- [ ] Define target label column (e.g., `Label` / `attack_cat`) and check class imbalance via `value_counts(normalize=True)`.\n",
    "- [ ] Double-check for nulls, duplicates, and obvious inconsistencies; visualize anomalies (e.g., `df.isna().sum().plot(kind=\"bar\")`).\n",
    "- [ ] Validate column types: convert numeric columns, standardize categorical dtypes, fix malformed values (ensure pandas/models interpret data correctly).\n",
    "- [ ] Split data using **Stratified Sampling** to maintain the same percentage of rare attack classes in train/validation/test sets.\n",
    "- [ ] Check feature correlations; remove or merge highly correlated columns to reduce redundancy and potential leakage.\n",
    "- [ ] Save train/validation/test splits as CSV files in a structured folder (separate for real and synthetic datasets) ‚Äî e.g., in Google Drive for reproducibility.\n",
    "- [ ] Data split percentages:\n",
    "  - **Real Data:** Training 80%, Validation 10%, Test 10%\n",
    "  - **Synthetic Data:** Training 10‚Äì20%, Validation 5%, Test 0%\n",
    "\n",
    "#### Secondary tasks (second essentials)\n",
    "- [ ] Produce a quick EDA snapshot (pairplots, histograms) to highlight feature distributions and potential distribution shifts.\n",
    "- [ ] Ensure deterministic splits: set a shared `RANDOM_STATE` (e.g., 42) in all splitters (`train_test_split`, `StratifiedKFold`) to guarantee reproducibility across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7665e",
   "metadata": {},
   "source": [
    "### SE2: DATA PREP FINAL\n",
    "\n",
    "#### colab : https://colab.research.google.com/drive/1YJh5SvbtX8iQJc6XtLddaj1fEOeMeBlX?usp=sharing\n",
    "\n",
    "#### drive : https://drive.google.com/drive/folders/1i3ZL1jffdgfY4O6AHmDrZRKY8WewUWTB?usp=sharing \n",
    " \n",
    " üì¢üì¢üì¢ (pls read):\n",
    "- ung ginawa ko sa synthetic data splitting para mahati: Count the Real Data > Calculate the Ratio > Apply Ratio to Synthetic Data. inapply ko yung ratio ng Real Data sa Synthetic splitting para ma-maximize yung training rows\n",
    "- did the splitting by using Stratified by 'Attack' instead of just 'Label' para maensure na ung mga rare attacks ay divided equally \n",
    "- ung sa synthetic data na naka split, inalign ko yung columns nun to 36 para same sa real data, I applied a filter to the synthetic splits to make sure they match exactly. This ensures na ung model won't crash due to a shape mismatch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72757dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. CLEAN THE DATA\n",
    "\n",
    "# Handle NaN values in real dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Start with a fresh copy\n",
    "data_clean = data.copy()\n",
    "\n",
    "# 2. Identify and Drop 'Noise' & 'Leakage' columns\n",
    "# (IPs, Ports, and exact Timestamps)\n",
    "cols_to_drop = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT', 'L4_DST_PORT',\n",
    "                'FLOW_START_MILLISECONDS', 'FLOW_END_MILLISECONDS']\n",
    "data_clean = data_clean.drop(columns=[c for c in cols_to_drop if c in data_clean.columns])\n",
    "print(\"‚úÖ Identification noise and timestamps removed.\")\n",
    "\n",
    "# 3. Memory Optimization (To prevent Colab from crashing later)\n",
    "for col in data_clean.select_dtypes(include=['int64']).columns:\n",
    "    data_clean[col] = pd.to_numeric(data_clean[col], downcast='integer')\n",
    "for col in data_clean.select_dtypes(include=['float64']).columns:\n",
    "    data_clean[col] = pd.to_numeric(data_clean[col], downcast='float')\n",
    "print(\"‚úÖ Memory optimized.\")\n",
    "\n",
    "# 4. Final Verification\n",
    "print(f\"\\nFinal Feature Count: {data_clean.shape[1]}\")\n",
    "print(\"Missing Values Check:\", data_clean.isnull().sum().sum())\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Missing values per column:\")\n",
    "print(\"=\"*60)\n",
    "missing_values = data_clean.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "    print(f\"\\nTotal missing values: {data_clean.isnull().sum().sum()}\")\n",
    "    print(f\"Percentage of data with missing values: {(data_clean.isnull().sum().sum() / (data_clean.shape[0] * data_clean.shape[1]) * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Data types:\")\n",
    "print(\"=\"*60)\n",
    "print(data_clean.dtypes.value_counts())\n",
    "\n",
    "# Check if IPs still exist\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IP columns still present?\")\n",
    "print(\"=\"*60)\n",
    "ip_columns = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR']\n",
    "for col in ip_columns:\n",
    "    if col in data_clean.columns:\n",
    "        print(f\"‚ùå {col}: Still present\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {col}: Already removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757cfd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. DROPPING NANS INSTEAD OF FILLING IT WITH 0 (it will create noise if filled with 0)\n",
    "\n",
    "#Dropping the NaN rows\n",
    "data_clean = data_clean.dropna(subset=['SRC_TO_DST_SECOND_BYTES'])\n",
    "\n",
    "#Final verification check\n",
    "print(f\"Rows remaining: {len(data_clean)}\")\n",
    "print(f\"Total missing values now: {data_clean.isnull().sum().sum()}\")\n",
    "\n",
    "if data_clean.isnull().sum().sum() == 0:\n",
    "    print(\"Dataset Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. CHECKS WHETHER THERE ARE SIMILAR COLUMNS\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "numeric_df = data_clean.select_dtypes(include=[np.number])\n",
    "\n",
    "corr_matrix = numeric_df.sample(100000, random_state=42).corr()\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"ForenXAI: Feature Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "upper = corr_matrix.abs().where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîé REDUNDANT COLUMNS DETECTED (>95% correlation):\")\n",
    "print(\"=\"*60)\n",
    "print(to_drop if to_drop else \"None! All features are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb05608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. DROPPED RELATED/SIMILAR COLUMNS\n",
    "\n",
    "# The list of redundant columns found\n",
    "redundant_cols = ['OUT_PKTS', 'CLIENT_TCP_FLAGS', 'SERVER_TCP_FLAGS', 'DURATION_IN',\n",
    "                  'MAX_TTL', 'MAX_IP_PKT_LEN', 'RETRANSMITTED_IN_BYTES',\n",
    "                  'RETRANSMITTED_IN_PKTS', 'RETRANSMITTED_OUT_BYTES',\n",
    "                  'RETRANSMITTED_OUT_PKTS', 'SRC_TO_DST_AVG_THROUGHPUT',\n",
    "                  'DST_TO_SRC_AVG_THROUGHPUT', 'ICMP_IPV4_TYPE']\n",
    "\n",
    "# Drop them from our clean dataset\n",
    "data_final = data_clean.drop(columns=redundant_cols)\n",
    "\n",
    "# check\n",
    "print(f\"Original Features: {data_clean.shape[1]}\")\n",
    "print(f\"Features after removing redundancy: {data_final.shape[1]}\")\n",
    "print(data_final.columns.tolist())\n",
    "print(\"‚úÖ Redundancy removed\")\n",
    "\n",
    "#Save the cleaned file\n",
    "output_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "\n",
    "print(f\"Saving file to: {output_path}...\")\n",
    "data_final.to_csv(output_path, index=False)\n",
    "print(\"SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which columns to validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"File loaded successfully: {len(df):,} rows and {len(df.columns)} columns.\")\n",
    "\n",
    "    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NON-NUMERIC COLUMNS FOUND:\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if non_numeric_cols:\n",
    "        for col in non_numeric_cols:\n",
    "            print(f\"- {col} (Type: {df[col].dtype})\")\n",
    "    else:\n",
    "        print(\"None! All columns are currently numerical.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLED VALUES FROM NON-NUMERIC COLUMNS:\")\n",
    "    print(\"=\"*50)\n",
    "    if non_numeric_cols:\n",
    "        print(df[non_numeric_cols].head(3))\n",
    "    else:\n",
    "        print(\"Everything is numeric.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the file at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Validation and Standardization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Standardize the Categorical Dtype\n",
    "# This changes 'object' to 'category'\n",
    "df['Attack'] = df['Attack'].astype('category')\n",
    "\n",
    "# check class imbalance via value_counts(normalize=True)c\n",
    "print(\"CLASS IMBALANCE (ATTACK CATEGORIES):\")\n",
    "print(\"=\"*50)\n",
    "print(df['Attack'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# visualize anomalies (e.g., df.isna().sum().plot(kind='bar'))\n",
    "plt.figure(figsize=(10, 6))\n",
    "df.isna().sum().plot(kind=\"bar\", color='skyblue')\n",
    "plt.title(\"Anomalies Check: Missing Values per Column\")\n",
    "plt.ylabel(\"Count of Nulls\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Type Validation and Standardization Complete.\")\n",
    "print(f\"Final Dtype for 'Attack': {df['Attack'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766aab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. DATA SPLITTING (REAL DATA 80/10/10)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING STRATIFIED SPLIT (80/10/10)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# peel off the 10% Test Set (The \"Final Exam\")\n",
    "# use 'stratify' to keep the 4% attack ratio perfect.\n",
    "X_temp, test_df = train_test_split(\n",
    "    data_final,\n",
    "    test_size=0.10,\n",
    "    stratify=data_final['Label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# split the remaining 90% into Train (80%) and Val (10%)\n",
    "# Math: 10% is 1/9th of the remaining 90%.\n",
    "train_df, val_df = train_test_split(\n",
    "    X_temp,\n",
    "    test_size=1/9,\n",
    "    stratify=X_temp['Attack'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create the Synthetic Seed (Essential for the \"Synthetic Data\" Task)\n",
    "synthetic_seed_df = train_df.copy()\n",
    "\n",
    "total_rows = len(data_final)\n",
    "\n",
    "print(f\"\\nüìä Split Results:\")\n",
    "print(f\"   - Total Data:      {total_rows:,} rows (100%)\")\n",
    "print(f\"   - Training Set:    {len(train_df):,} rows ({len(train_df)/total_rows:.1%}) --> Matches 'Training > 80%'\")\n",
    "print(f\"   - Validation Set:  {len(val_df):,} rows ({len(val_df)/total_rows:.1%}) --> Matches 'Validation > 90%'\")\n",
    "print(f\"   - Test Set:        {len(test_df):,} rows ({len(test_df)/total_rows:.1%}) --> Matches 'Test > 95%'\")\n",
    "\n",
    "# Check Stratification\n",
    "attack_ratio_train = train_df['Label'].mean()\n",
    "attack_ratio_test = test_df['Label'].mean()\n",
    "print(f\"\\n‚öñÔ∏è Stratification Check (Attack Ratio should be ~0.039):\")\n",
    "print(f\"   - Train Attack Ratio: {attack_ratio_train:.5f}\")\n",
    "print(f\"   - Test Attack Ratio:  {attack_ratio_test:.5f}\")\n",
    "\n",
    "if abs(attack_ratio_train - attack_ratio_test) < 0.001:\n",
    "    print(\"PERFECT STRATIFICATION CONFIRMED\")\n",
    "else:\n",
    "    print(\"arning: Stratification might be off.\")\n",
    "\n",
    "output_dir = '/content/drive/MyDrive/ForenXAI_cleaned/'\n",
    "\n",
    "print(f\"\\nüíæ Saving files to {output_dir}...\")\n",
    "train_df.to_csv(f'{output_dir}train_real.csv', index=False)\n",
    "val_df.to_csv(f'{output_dir}val_real.csv', index=False)\n",
    "test_df.to_csv(f'{output_dir}test_real.csv', index=False)\n",
    "synthetic_seed_df.to_csv(f'{output_dir}synthetic_seed.csv', index=False)\n",
    "\n",
    "print(\"SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe46f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Peel off the 10% Test Set (The \"Final Exam\")\n",
    "X_temp, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.10,\n",
    "    stratify=df['Attack'], # Preserves the rare 0.005% Worms in all sets\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Split remaining 90% into Train (80% total) and Val (10% total)\n",
    "train_df, val_df = train_test_split(\n",
    "    X_temp,\n",
    "    test_size=1/9,\n",
    "    stratify=X_temp['Attack'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Save the final files to your Drive\n",
    "output_dir = '/content/drive/MyDrive/ForenXAI_cleaned/'\n",
    "\n",
    "train_df.to_csv(f'{output_dir}train_real.csv', index=False)\n",
    "val_df.to_csv(f'{output_dir}val_real.csv', index=False)\n",
    "test_df.to_csv(f'{output_dir}test_real.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL SPLIT COMPLETE (Stratified by Attack Category)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training rows:   {len(train_df):,}\")\n",
    "print(f\"Validation rows: {len(val_df):,}\")\n",
    "print(f\"Test rows:       {len(test_df):,}\")\n",
    "print(f\"Files saved to:  {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f568a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'Label' is treated as a string for the legend colors\n",
    "df['Label'] = df['Label'].astype(str)\n",
    "\n",
    "# Select Key Features\n",
    "key_features = ['IN_BYTES', 'OUT_BYTES', 'FLOW_DURATION_MILLISECONDS',\n",
    "                'IN_PKTS', 'TCP_WIN_MAX_IN', 'MIN_TTL']\n",
    "\n",
    "print(\"Generating Histograms (handling zeros)...\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(key_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "\n",
    "    # add 1 to the data (df[feature] + 1) just for the plot.\n",
    "    # This turns 0 into 1, so log(1) = 0.\n",
    "    sns.histplot(data=df, x=df[feature] + 1, hue='Label', bins=30,\n",
    "                 kde=True, palette='viridis', log_scale=True)\n",
    "\n",
    "    plt.title(f'Distribution of {feature} (Log Scale)')\n",
    "    plt.xlabel(f'{feature} (+1 for log)')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/ForenXAI_cleaned/eda_histograms.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Generating Pairplot (using sample)...\")\n",
    "\n",
    "# Create Pairplot\n",
    "# applied log+1 here to keep the visual relationships consistent\n",
    "plot_df = df.sample(5000, random_state=42).copy()\n",
    "for feat in ['IN_BYTES', 'OUT_BYTES', 'FLOW_DURATION_MILLISECONDS', 'IN_PKTS']:\n",
    "    plot_df[feat] = np.log1p(plot_df[feat]) # log1p is shorthand for log(1+x)\n",
    "\n",
    "sns.pairplot(plot_df,\n",
    "             vars=['IN_BYTES', 'OUT_BYTES', 'FLOW_DURATION_MILLISECONDS', 'IN_PKTS'],\n",
    "             hue='Label',\n",
    "             palette='husl',\n",
    "             diag_kind='kde',\n",
    "             plot_kws={'alpha': 0.4})\n",
    "\n",
    "plt.suptitle(\"Feature Interactions Snapshot (Log-Transformed)\", y=1.02)\n",
    "plt.savefig('/content/drive/MyDrive/ForenXAI_cleaned/eda_pairplot.png')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Check: {df['IN_BYTES'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4692418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOUBLE CHECKING CLEANED DATA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "\n",
    "try:\n",
    "    data_final = pd.read_csv(file_path)\n",
    "    print(\"File loaded successfully.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"--- CLEANING VERIFICATION REPORT ---\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Check for NaNs\n",
    "    nan_count = data_final.isnull().sum().sum()\n",
    "    print(f\"Total Missing Values:  {nan_count} {'(Clean)' if nan_count == 0 else '(Action Required)'}\")\n",
    "\n",
    "    # Check for Identifiers (Should be gone)\n",
    "    id_cols = ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR', 'L4_SRC_PORT', 'L4_DST_PORT', 'Timestamp']\n",
    "    present_ids = [col for col in id_cols if col in data_final.columns]\n",
    "    if not present_ids:\n",
    "        print(f\"Identifier Columns:    (Removed)\")\n",
    "    else:\n",
    "        print(f\"Identifier Columns:    Found: {present_ids}\")\n",
    "\n",
    "    # Check Feature Count (Started ~55 -> Dropped 6 IDs -> Dropped 13 Redundant -> Goal 36)\n",
    "    col_count = data_final.shape[1]\n",
    "    expected = 36\n",
    "    if col_count == expected:\n",
    "         print(f\"Final Feature Count:   {col_count} (Matches Expected)\")\n",
    "    else:\n",
    "         print(f\"Final Feature Count:   {col_count} (Expected {expected})\")\n",
    "\n",
    "    # Check Target Columns Existence\n",
    "    if 'Label' in data_final.columns and 'Attack' in data_final.columns:\n",
    "        print(f\"Target Columns:        Found 'Label' and 'Attack'\")\n",
    "    else:\n",
    "        print(f\"Target Columns:        Missing Target Columns!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: Could not find file at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b787bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "print(f\"Loading data from {file_path}...\")\n",
    "data_final = pd.read_csv(file_path)\n",
    "\n",
    "target_column = 'Label'\n",
    "current_data = data_final\n",
    "\n",
    "# Count class distribution\n",
    "class_counts = current_data[target_column].value_counts().sort_index()\n",
    "\n",
    "print(\"\\n--- CLEANED DATA DISTRIBUTION ---\")\n",
    "print(f\"Total samples: {len(current_data):,}\")\n",
    "print(\"\\nClass proportions:\")\n",
    "print(current_data[target_column].value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Bar Chart\n",
    "plt.subplot(1, 2, 1)\n",
    "class_counts.plot(kind='bar', color='seagreen', edgecolor='black')\n",
    "plt.title('Class Distribution (Cleaned Data)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Class (0=Normal, 1=Attack)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, v in enumerate(class_counts):\n",
    "    plt.text(i, v + max(class_counts)*0.01, f\"{v:,}\", ha='center', va='bottom')\n",
    "\n",
    "# Pie Chart\n",
    "plt.subplot(1, 2, 2)\n",
    "# Using generic colors to differentiate from the earlier \"Attack Category\" charts\n",
    "plt.pie(class_counts, labels=['Normal (0)', 'Attack (1)'], autopct='%1.2f%%',\n",
    "        startangle=90, colors=['#99ff99','#ff9999'])\n",
    "plt.title('Cleaned Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imbalance metrics\n",
    "max_class = class_counts.max()\n",
    "min_class = class_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"UPDATED IMBALANCE ANALYSIS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"New Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "print(f\"Most common class: {class_counts.idxmax()} ({max_class:,} samples)\")\n",
    "print(f\"Least common class: {class_counts.idxmin()} ({min_class:,} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae57c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both just to peek at the columns\n",
    "real_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "syn_path = '/content/drive/MyDrive/ForenXAI_Datasets/Synthetic_Datasets/synthetic_data_ctgan.csv'\n",
    "\n",
    "try:\n",
    "    real_df = pd.read_csv(real_path)\n",
    "    syn_df = pd.read_csv(syn_path)\n",
    "\n",
    "    real_cols = set(real_df.columns)\n",
    "    syn_cols = set(syn_df.columns)\n",
    "\n",
    "    print(f\"Real Data Columns:      {len(real_cols)}\")\n",
    "    print(f\"Synthetic Data Columns: {len(syn_cols)}\")\n",
    "\n",
    "    # Check for mismatch\n",
    "    if real_cols == syn_cols:\n",
    "        print(\"\\ncolumns match perfectly!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è MISMATCH DETECTED\")\n",
    "        print(f\"Columns in Synthetic but NOT in Real: {syn_cols - real_cols}\")\n",
    "        print(f\"Columns in Real but NOT in Synthetic: {real_cols - syn_cols}\")\n",
    "        print(\"\\nVERDICT: You MUST run the 'alignment' code, or the model training will crash.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Files not found. Check paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. SYNTHETIC SPLITTNG\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "syn_path = '/content/drive/MyDrive/ForenXAI_Datasets/Synthetic_Datasets/synthetic_data_ctgan.csv'\n",
    "real_clean_path = '/content/drive/MyDrive/ForenXAI_cleaned/cleaned_real_data_final.csv'\n",
    "output_dir = '/content/drive/MyDrive/ForenXAI_cleaned/'\n",
    "\n",
    "print(\"Loading Data for Alignment...\")\n",
    "try:\n",
    "    syn_df = pd.read_csv(syn_path)\n",
    "    real_df = pd.read_csv(real_clean_path)\n",
    "\n",
    "    print(f\"Original Synthetic Shape: {syn_df.shape}\")\n",
    "\n",
    "    # ALIGNMENT STEP (The Fix)\n",
    "    # This prevents the 'Shape Mismatch' error during training.\n",
    "    cols_to_keep = [c for c in real_df.columns if c in syn_df.columns]\n",
    "    syn_df = syn_df[cols_to_keep]\n",
    "\n",
    "    print(f\"Aligned Synthetic Shape:  {syn_df.shape} (Matches Real Data)\")\n",
    "\n",
    "    # STANDARDIZE ATTACK COLUMN\n",
    "    # If the synthetic data has 'Attack', make sure it's a category like the real data\n",
    "    if 'Attack' in syn_df.columns:\n",
    "        syn_df['Attack'] = syn_df['Attack'].astype('category')\n",
    "\n",
    "    # STRATIFIED SPLIT\n",
    "    # use 'Attack' if available, otherwise fallback to 'Label'\n",
    "    strat_col = syn_df['Attack'] if 'Attack' in syn_df.columns else syn_df['Label']\n",
    "\n",
    "    train_ratio = 0.8889 # Matches the proportion of Real Data\n",
    "    print(f\"Splitting... (Stratifying by {strat_col.name})\")\n",
    "\n",
    "    syn_train, syn_val = train_test_split(\n",
    "        syn_df,\n",
    "        train_size=train_ratio,\n",
    "        stratify=strat_col,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    syn_train.to_csv(f'{output_dir}synthetic_train_split.csv', index=False)\n",
    "    syn_val.to_csv(f'{output_dir}synthetic_val_split.csv', index=False)\n",
    "\n",
    "    print(f\"SUCCESS. Files saved to: {output_dir}\")\n",
    "    print(f\"- synthetic_train_split.csv ({len(syn_train)} rows)\")\n",
    "    print(f\"- synthetic_val_split.csv ({len(syn_val)} rows)\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
