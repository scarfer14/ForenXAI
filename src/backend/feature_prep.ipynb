{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bc1987",
   "metadata": {},
   "source": [
    "### TODO: Feature Preparation and Transformation\n",
    "\n",
    "#### Primary tasks (must-have)\n",
    "- [ ] Combine training data: `train_real + train_synthetic`\n",
    "- [ ] Feature engineering → create new features (e.g., ratios, aggregations, time-based features)\n",
    "- [ ] Encoding categorical variables → numeric (e.g., OneHotEncoder, LabelEncoder)\n",
    "- [ ] Feature transformation → normalize/skew fixes (e.g., log transform, Box-Cox)\n",
    "- [ ] Feature scaling → min-max or standardize numeric features\n",
    "- [ ] Save engineered training features to CSV or joblib for reproducibility\n",
    "\n",
    "#### Secondary tasks (second essentials)\n",
    "- [ ] [Supervised ML] Preserve TF-IDF vocabulary/features used by the RandomForest text pipeline so SHAP explains identical dimensions (Tip: dump `tfidf.get_feature_names_out()` with the encoder artifacts)\n",
    "- [ ] [Unsupervised ML] Assemble the Isolation-Forest feature frame (`Hour`, `Src_IP_LastOctet`, etc.) and record any imputations so runtime anomaly detection feeds match training\n",
    "- [ ] Optional: Visualize distributions of engineered features (histograms, boxplots) to detect outliers or anomalies\n",
    "\n",
    "#### Reminder\n",
    "- [ ] Training Data → Apply full Feature Engineering pipeline → Train Model\n",
    "- [ ] Validation/Test Data → Apply **same transformations fitted on training data** → Evaluate Model\n",
    "- [ ] Persist all preprocessing objects (scalers, encoders, transformers) for downstream inference\n",
    "- [ ] Training Dataset > fit + transform\n",
    "- [ ] Validation & Test > transform only\n" 

   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
