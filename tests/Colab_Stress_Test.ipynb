{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c6146c",
   "metadata": {},
   "source": [
    "# ForenXAI Model Stress Testing - Google Colab\n",
    "\n",
    "**Comprehensive stress testing using real synthetic network traffic data**\n",
    "\n",
    "This notebook:\n",
    "- Loads trained models from Google Drive\n",
    "- Uses synthetic CSV data for realistic testing\n",
    "- Measures performance (throughput, latency, accuracy)\n",
    "- Provides detailed results with visualizations\n",
    "\n",
    "**Setup Requirements:**\n",
    "1. Upload models to: `My Drive/Featured Dataset/trained_models/`\n",
    "2. Upload CSV to: `My Drive/Featured Dataset/processed/`\n",
    "3. Run all cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19084902",
   "metadata": {},
   "source": [
    "## 1. Setup & Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83514e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q scikit-learn tensorflow pandas numpy joblib psutil\n",
    "\n",
    "print(\"‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025f6710",
   "metadata": {},
   "source": [
    "## 2. Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45109006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths (adjust if your folders are named differently)\n",
    "MODELS_DIR = '/content/drive/MyDrive/Featured Dataset/trained_models'\n",
    "DATA_DIR = '/content/drive/MyDrive/Featured Dataset/processed'\n",
    "\n",
    "# CSV file to use for testing\n",
    "TEST_CSV = 'synthetic_train_split.csv'  # or 'synthetic_val_split.csv'\n",
    "\n",
    "# Model files\n",
    "RF_MODEL = 'random_forest_pipeline.joblib'\n",
    "MLP_MODEL = 'mlp_model.h5'\n",
    "ISO_MODEL = 'isolation_forest_pipeline.joblib'\n",
    "\n",
    "print(f\"üìÇ Models Directory: {MODELS_DIR}\")\n",
    "print(f\"üìÇ Data Directory: {DATA_DIR}\")\n",
    "print(f\"üìÑ Test CSV: {TEST_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files exist\n",
    "csv_path = os.path.join(DATA_DIR, TEST_CSV)\n",
    "rf_path = os.path.join(MODELS_DIR, RF_MODEL)\n",
    "mlp_path = os.path.join(MODELS_DIR, MLP_MODEL)\n",
    "iso_path = os.path.join(MODELS_DIR, ISO_MODEL)\n",
    "\n",
    "print(\"\\nüîç Checking files...\")\n",
    "print(f\"CSV File: {'‚úÖ' if os.path.exists(csv_path) else '‚ùå'} {csv_path}\")\n",
    "print(f\"RF Model: {'‚úÖ' if os.path.exists(rf_path) else '‚ùå'} {rf_path}\")\n",
    "print(f\"MLP Model: {'‚úÖ' if os.path.exists(mlp_path) else '‚ùå'} {mlp_path}\")\n",
    "print(f\"ISO Model: {'‚úÖ' if os.path.exists(iso_path) else '‚ùå'} {iso_path}\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"\\n‚ö†Ô∏è CSV file not found! Upload synthetic_train_split.csv to your Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb574c1f",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7db27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "print(\"üìä Loading data from CSV...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} samples\")\n",
    "print(f\"   Features: {df.shape[1] - 2} (excluding Label and Attack)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec5356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data distribution\n",
    "print(\"üìà Data Distribution:\")\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"\\nAttack Types:\")\n",
    "print(df['Attack'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad751382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "print(\"üîß Preparing features and labels...\")\n",
    "\n",
    "# Extract features (drop Label and Attack columns)\n",
    "X = df.drop(columns=['Label', 'Attack'], errors='ignore').values\n",
    "\n",
    "# Extract labels (0=Normal/Benign, 1=Attack)\n",
    "y = df['Label'].values\n",
    "\n",
    "# Extract attack types\n",
    "attack_types = df['Attack'].values\n",
    "\n",
    "# Clean data (handle NaN and Inf)\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "print(f\"‚úÖ Prepared {len(X):,} samples\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Labels shape: {y.shape}\")\n",
    "print(f\"   Normal: {np.sum(y==0):,} ({np.mean(y==0)*100:.1f}%)\")\n",
    "print(f\"   Attack: {np.sum(y==1):,} ({np.mean(y==1)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7665edf",
   "metadata": {},
   "source": [
    "## 4. Sample Data for Faster Testing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster testing, sample a subset\n",
    "# Set to None to use all data\n",
    "SAMPLE_SIZE = 10000  # Change to None for full dataset\n",
    "\n",
    "if SAMPLE_SIZE and SAMPLE_SIZE < len(X):\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(len(X), SAMPLE_SIZE, replace=False)\n",
    "    X_test = X[indices]\n",
    "    y_test = y[indices]\n",
    "    attack_test = attack_types[indices]\n",
    "    print(f\"üì¶ Using {SAMPLE_SIZE:,} samples for testing\")\n",
    "else:\n",
    "    X_test = X\n",
    "    y_test = y\n",
    "    attack_test = attack_types\n",
    "    print(f\"üì¶ Using all {len(X):,} samples for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45c8c0",
   "metadata": {},
   "source": [
    "## 5. Define Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, model, X, y, attack_types):\n",
    "    \"\"\"\n",
    "    Test a model and return comprehensive metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TESTING: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Measure memory before\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    \n",
    "    # Make predictions with timing\n",
    "    print(f\"\\n‚è±Ô∏è  Making {len(X):,} predictions...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if model_name == 'MLP Neural Network':\n",
    "        predictions_proba = model.predict(X, verbose=0).flatten()\n",
    "        predictions = (predictions_proba > 0.5).astype(int)\n",
    "    elif model_name == 'Isolation Forest':\n",
    "        preds = model.predict(X)\n",
    "        predictions = np.where(preds == -1, 1, 0)\n",
    "    else:\n",
    "        predictions = model.predict(X)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Measure memory after\n",
    "    mem_after = process.memory_info().rss / 1024 / 1024\n",
    "    \n",
    "    # Calculate metrics\n",
    "    throughput = len(X) / elapsed_time\n",
    "    latency = (elapsed_time / len(X)) * 1000  # ms per sample\n",
    "    \n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions, zero_division=0)\n",
    "    recall = recall_score(y, predictions, zero_division=0)\n",
    "    f1 = f1_score(y, predictions, zero_division=0)\n",
    "    \n",
    "    cm = confusion_matrix(y, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n‚ö° PERFORMANCE:\")\n",
    "    print(f\"   Time: {elapsed_time:.3f} seconds\")\n",
    "    print(f\"   Throughput: {throughput:,.0f} samples/sec\")\n",
    "    print(f\"   Latency: {latency:.4f} ms/sample\")\n",
    "    print(f\"   Memory: {mem_after - mem_before:.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nüìä ACCURACY:\")\n",
    "    print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall:    {recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ CONFUSION MATRIX:\")\n",
    "    print(f\"   True Negatives:  {tn:,}\")\n",
    "    print(f\"   False Positives: {fp:,}\")\n",
    "    print(f\"   False Negatives: {fn:,}\")\n",
    "    print(f\"   True Positives:  {tp:,}\")\n",
    "    \n",
    "    # Attack type breakdown\n",
    "    print(f\"\\nüîç ATTACK TYPE BREAKDOWN:\")\n",
    "    for attack in np.unique(attack_types):\n",
    "        mask = attack_types == attack\n",
    "        if np.sum(mask) > 0:\n",
    "            attack_acc = accuracy_score(y[mask], predictions[mask])\n",
    "            print(f\"   {attack:<20}: {attack_acc:.4f} ({np.sum(mask):,} samples)\")\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'samples': len(X),\n",
    "        'time': elapsed_time,\n",
    "        'throughput': throughput,\n",
    "        'latency': latency,\n",
    "        'memory_mb': mem_after - mem_before,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'tn': int(tn),\n",
    "        'fp': int(fp),\n",
    "        'fn': int(fn),\n",
    "        'tp': int(tp)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Testing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96f5cc0",
   "metadata": {},
   "source": [
    "## 6. Test Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90068664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test Random Forest\n",
    "if os.path.exists(rf_path):\n",
    "    print(\"üì¶ Loading Random Forest model...\")\n",
    "    rf_model = joblib.load(rf_path)\n",
    "    print(\"‚úÖ Model loaded!\")\n",
    "    \n",
    "    rf_results = test_model('Random Forest', rf_model, X_test, y_test, attack_test)\n",
    "else:\n",
    "    print(\"‚ùå Random Forest model not found\")\n",
    "    rf_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519907f",
   "metadata": {},
   "source": [
    "## 7. Test MLP Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test MLP\n",
    "if os.path.exists(mlp_path):\n",
    "    print(\"üì¶ Loading MLP Neural Network model...\")\n",
    "    mlp_model = load_model(mlp_path, compile=False)\n",
    "    print(\"‚úÖ Model loaded!\")\n",
    "    \n",
    "    mlp_results = test_model('MLP Neural Network', mlp_model, X_test, y_test, attack_test)\n",
    "else:\n",
    "    print(\"‚ùå MLP model not found\")\n",
    "    mlp_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936675ef",
   "metadata": {},
   "source": [
    "## 8. Test Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a270fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test Isolation Forest\n",
    "if os.path.exists(iso_path):\n",
    "    print(\"üì¶ Loading Isolation Forest model...\")\n",
    "    iso_model = joblib.load(iso_path)\n",
    "    print(\"‚úÖ Model loaded!\")\n",
    "    \n",
    "    iso_results = test_model('Isolation Forest', iso_model, X_test, y_test, attack_test)\n",
    "else:\n",
    "    print(\"‚ùå Isolation Forest model not found\")\n",
    "    iso_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00b94d",
   "metadata": {},
   "source": [
    "## 9. Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "all_results = [r for r in [rf_results, mlp_results, iso_results] if r is not None]\n",
    "\n",
    "if all_results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STRESS TEST SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison table\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(f\"\\n{'Model':<25} {'Throughput':<18} {'Latency':<15} {'Accuracy':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\"*82)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"{row['model']:<25} {row['throughput']:>8,.0f} s/s      \"\n",
    "              f\"{row['latency']:>6.4f} ms    \"\n",
    "              f\"{row['accuracy']:>6.4f}      \"\n",
    "              f\"{row['f1_score']:>6.4f}\")\n",
    "    \n",
    "    # Best performers\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST PERFORMERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_acc = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "    best_throughput = results_df.loc[results_df['throughput'].idxmax()]\n",
    "    best_f1 = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "    \n",
    "    print(f\"üéØ Best Accuracy:  {best_acc['model']} ({best_acc['accuracy']:.4f})\")\n",
    "    print(f\"‚ö° Best Throughput: {best_throughput['model']} ({best_throughput['throughput']:,.0f} s/s)\")\n",
    "    print(f\"üìä Best F1-Score:  {best_f1['model']} ({best_f1['f1_score']:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ STRESS TESTING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display full results dataframe\n",
    "    print(\"\\nüìä Detailed Results:\")\n",
    "    results_df\n",
    "else:\n",
    "    print(\"\\n‚ùå No models were tested. Check that models are uploaded to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a2a15",
   "metadata": {},
   "source": [
    "## 10. Save Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "if all_results:\n",
    "    output_path = '/content/drive/MyDrive/stress_test_results.csv'\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e2062",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes:\n",
    "\n",
    "**Interpreting Results:**\n",
    "- **Throughput**: Higher is better (>5,000 s/s is excellent)\n",
    "- **Latency**: Lower is better (<0.5 ms is excellent)\n",
    "- **Accuracy**: Higher is better (>95% is good, >98% is excellent)\n",
    "- **F1-Score**: Balanced metric (>0.90 is good, >0.95 is excellent)\n",
    "\n",
    "**What to Check:**\n",
    "1. All models should have accuracy >90%\n",
    "2. F1-Score should be >0.85 for production\n",
    "3. Throughput should be >1,000 samples/sec\n",
    "4. False negatives (missed attacks) should be minimized\n",
    "\n",
    "**Next Steps:**\n",
    "- If results look good: Models are production-ready ‚úÖ\n",
    "- If accuracy is low: Retrain models with more data\n",
    "- If throughput is low: Consider model optimization or hardware upgrade"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
